apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: enforce-gpu-node-usage
  annotations:
    policies.kyverno.io/title: Enforce GPU Node Usage
    policies.kyverno.io/category: Resource Management, GPU
    policies.kyverno.io/severity: high
    policies.kyverno.io/subject: Pod
    policies.kyverno.io/description: >-
      Ensures that Pods requesting GPU resources are properly scheduled on GPU nodes
      and have appropriate node selectors, tolerations, and resource limits.
spec:
  validationFailureAction: Enforce
  background: true
  failurePolicy: Fail
  rules:
  - name: require-gpu-node-selector
    match:
      any:
      - resources:
          kinds:
          - Pod
    preconditions:
      any:
      - key: "{{ request.object.spec.containers[].resources.limits.\"nvidia.com/gpu\" || '0' }}"
        operator: NotEquals
        value: "0"
    validate:
      message: >-
        Pods requesting NVIDIA GPUs must have nodeSelector 'node-type: gpu' or 'gpu-type: nvidia'.
      anyPattern:
      - spec:
          nodeSelector:
            node-type: gpu
      - spec:
          nodeSelector:
            gpu-type: nvidia
      - spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: node-type
                    operator: In
                    values:
                    - gpu
  - name: require-gpu-toleration
    match:
      any:
      - resources:
          kinds:
          - Pod
    preconditions:
      any:
      - key: "{{ request.object.spec.containers[].resources.limits.\"nvidia.com/gpu\" || '0' }}"
        operator: NotEquals
        value: "0"
    validate:
      message: >-
        Pods requesting GPUs must have toleration for 'nvidia.com/gpu=present:NoSchedule'.
      deny:
        conditions:
          any:
          - key: "{{ request.object.spec.tolerations[?key=='nvidia.com/gpu'] || '' }}"
            operator: Equals
            value: ""
  - name: validate-gpu-limits
    match:
      any:
      - resources:
          kinds:
          - Pod
    preconditions:
      any:
      - key: "{{ request.object.spec.containers[].resources.limits.\"nvidia.com/gpu\" || '0' }}"
        operator: NotEquals
        value: "0"
    validate:
      message: >-
        GPU requests must match GPU limits. Set both requests and limits to the same value.
      foreach:
      - list: "request.object.spec.containers[]"
        deny:
          conditions:
            any:
            - key: "{{ element.resources.limits.\"nvidia.com/gpu\" || '0' }}"
              operator: NotEquals
              value: "{{ element.resources.requests.\"nvidia.com/gpu\" || '0' }}"
  - name: auto-add-gpu-config
    match:
      any:
      - resources:
          kinds:
          - Pod
    preconditions:
      any:
      - key: "{{ request.object.spec.containers[].resources.limits.\"nvidia.com/gpu\" || '0' }}"
        operator: NotEquals
        value: "0"
    mutate:
      patchStrategicMerge:
        spec:
          nodeSelector:
            +(node-type): gpu
            +(gpu-type): nvidia
          tolerations:
          - +(key): nvidia.com/gpu
            +(operator): Equal
            +(value): present
            +(effect): NoSchedule
---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: validate-gpu-workload-labels
  annotations:
    policies.kyverno.io/title: Validate GPU Workload Labels
    policies.kyverno.io/category: Best Practices, GPU
    policies.kyverno.io/severity: medium
    policies.kyverno.io/description: >-
      Ensures GPU workloads have appropriate labels for tracking and monitoring.
spec:
  validationFailureAction: Audit
  background: true
  rules:
  - name: require-workload-label
    match:
      any:
      - resources:
          kinds:
          - Pod
    preconditions:
      any:
      - key: "{{ request.object.spec.containers[].resources.limits.\"nvidia.com/gpu\" || '0' }}"
        operator: NotEquals
        value: "0"
    validate:
      message: >-
        GPU workloads should have label 'workload' set to identify the type
        (e.g., training, inference, rendering).
      pattern:
        metadata:
          labels:
            workload: "?*"
  - name: recommend-cost-center
    match:
      any:
      - resources:
          kinds:
          - Pod
    preconditions:
      any:
      - key: "{{ request.object.spec.containers[].resources.limits.\"nvidia.com/gpu\" || '0' }}"
        operator: NotEquals
        value: "0"
    validate:
      message: >-
        GPU workloads should include 'cost-center' label for billing and cost tracking.
      pattern:
        metadata:
          =(labels):
            =(cost-center): "?*"
